{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "169ae07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (2.9.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (14.0.6)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.26.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.10,>=2.9 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.9.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: flatbuffers<2,>=1.12 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.12)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.19.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.12.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.21.5)\n",
      "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (2.9.0)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.42.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.33.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.2.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow) (3.0.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f61adb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: keras in c:\\users\\jmjun\\appdata\\roaming\\python\\python39\\site-packages (2.9.0)\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7169c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.layers import LSTM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9de0868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt=pd.read_excel('C:\\\\Users\\\\jmjun\\\\Desktop\\\\Bigreader\\\\KODATA\\\\data\\\\pt_insuarance.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07159236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>마감년월</th>\n",
       "      <th>피보험자수</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015년11월</td>\n",
       "      <td>120088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015년12월</td>\n",
       "      <td>120295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016년01월</td>\n",
       "      <td>119858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016년02월</td>\n",
       "      <td>120356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016년03월</td>\n",
       "      <td>121078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016년04월</td>\n",
       "      <td>121528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2016년05월</td>\n",
       "      <td>122206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2016년06월</td>\n",
       "      <td>122970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2016년07월</td>\n",
       "      <td>122796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2016년08월</td>\n",
       "      <td>122465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2016년09월</td>\n",
       "      <td>123690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2016년10월</td>\n",
       "      <td>124336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2016년11월</td>\n",
       "      <td>124824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2016년12월</td>\n",
       "      <td>124628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2017년01월</td>\n",
       "      <td>124588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2017년02월</td>\n",
       "      <td>125185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2017년03월</td>\n",
       "      <td>125747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2017년04월</td>\n",
       "      <td>126607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2017년05월</td>\n",
       "      <td>126203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2017년06월</td>\n",
       "      <td>127166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2017년07월</td>\n",
       "      <td>127229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017년08월</td>\n",
       "      <td>127447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2017년09월</td>\n",
       "      <td>128266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>2017년10월</td>\n",
       "      <td>128593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2017년11월</td>\n",
       "      <td>129329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2017년12월</td>\n",
       "      <td>130058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2018년01월</td>\n",
       "      <td>129969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2018년02월</td>\n",
       "      <td>128325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2018년03월</td>\n",
       "      <td>129041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2018년04월</td>\n",
       "      <td>129476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2018년05월</td>\n",
       "      <td>129897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>2018년06월</td>\n",
       "      <td>129605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2018년07월</td>\n",
       "      <td>129796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2018년08월</td>\n",
       "      <td>141249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2018년09월</td>\n",
       "      <td>141673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2018년10월</td>\n",
       "      <td>142398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2018년11월</td>\n",
       "      <td>142711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>2018년12월</td>\n",
       "      <td>142574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>2019년01월</td>\n",
       "      <td>142156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2019년02월</td>\n",
       "      <td>143540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>2019년03월</td>\n",
       "      <td>144305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>2019년04월</td>\n",
       "      <td>145096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2019년05월</td>\n",
       "      <td>145434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2019년06월</td>\n",
       "      <td>145599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>2019년07월</td>\n",
       "      <td>146164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>2019년08월</td>\n",
       "      <td>146310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>2019년09월</td>\n",
       "      <td>146479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>2019년10월</td>\n",
       "      <td>146708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2019년11월</td>\n",
       "      <td>147131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>2019년12월</td>\n",
       "      <td>146833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>2020년01월</td>\n",
       "      <td>145938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2020년02월</td>\n",
       "      <td>146460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>2020년03월</td>\n",
       "      <td>146752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2020년04월</td>\n",
       "      <td>147124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>2020년05월</td>\n",
       "      <td>147708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>2020년06월</td>\n",
       "      <td>148346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2020년07월</td>\n",
       "      <td>148753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        마감년월   피보험자수\n",
       "0   2015년11월  120088\n",
       "1   2015년12월  120295\n",
       "2   2016년01월  119858\n",
       "3   2016년02월  120356\n",
       "4   2016년03월  121078\n",
       "5   2016년04월  121528\n",
       "6   2016년05월  122206\n",
       "7   2016년06월  122970\n",
       "8   2016년07월  122796\n",
       "9   2016년08월  122465\n",
       "10  2016년09월  123690\n",
       "11  2016년10월  124336\n",
       "12  2016년11월  124824\n",
       "13  2016년12월  124628\n",
       "14  2017년01월  124588\n",
       "15  2017년02월  125185\n",
       "16  2017년03월  125747\n",
       "17  2017년04월  126607\n",
       "18  2017년05월  126203\n",
       "19  2017년06월  127166\n",
       "20  2017년07월  127229\n",
       "21  2017년08월  127447\n",
       "22  2017년09월  128266\n",
       "23  2017년10월  128593\n",
       "24  2017년11월  129329\n",
       "25  2017년12월  130058\n",
       "26  2018년01월  129969\n",
       "27  2018년02월  128325\n",
       "28  2018년03월  129041\n",
       "29  2018년04월  129476\n",
       "30  2018년05월  129897\n",
       "31  2018년06월  129605\n",
       "32  2018년07월  129796\n",
       "33  2018년08월  141249\n",
       "34  2018년09월  141673\n",
       "35  2018년10월  142398\n",
       "36  2018년11월  142711\n",
       "37  2018년12월  142574\n",
       "38  2019년01월  142156\n",
       "39  2019년02월  143540\n",
       "40  2019년03월  144305\n",
       "41  2019년04월  145096\n",
       "42  2019년05월  145434\n",
       "43  2019년06월  145599\n",
       "44  2019년07월  146164\n",
       "45  2019년08월  146310\n",
       "46  2019년09월  146479\n",
       "47  2019년10월  146708\n",
       "48  2019년11월  147131\n",
       "49  2019년12월  146833\n",
       "50  2020년01월  145938\n",
       "51  2020년02월  146460\n",
       "52  2020년03월  146752\n",
       "53  2020년04월  147124\n",
       "54  2020년05월  147708\n",
       "55  2020년06월  148346\n",
       "56  2020년07월  148753"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0739dbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "# y = np.array([1, 2, 3, 4, 5, 6])\n",
    "X_pt=np.array(pt.피보험자수)\n",
    "y_pt= np.array(pt.피보험자수[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d89ada49",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt=X_pt[:56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7389636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120088, 120295, 119858, 120356, 121078, 121528, 122206, 122970,\n",
       "       122796, 122465, 123690, 124336, 124824, 124628, 124588, 125185,\n",
       "       125747, 126607, 126203, 127166, 127229, 127447, 128266, 128593,\n",
       "       129329, 130058, 129969, 128325, 129041, 129476, 129897, 129605,\n",
       "       129796, 141249, 141673, 142398, 142711, 142574, 142156, 143540,\n",
       "       144305, 145096, 145434, 145599, 146164, 146310, 146479, 146708,\n",
       "       147131, 146833, 145938, 146460, 146752, 147124, 147708, 148346],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a9a17dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([120295, 119858, 120356, 121078, 121528, 122206, 122970, 122796,\n",
       "       122465, 123690, 124336, 124824, 124628, 124588, 125185, 125747,\n",
       "       126607, 126203, 127166, 127229, 127447, 128266, 128593, 129329,\n",
       "       130058, 129969, 128325, 129041, 129476, 129897, 129605, 129796,\n",
       "       141249, 141673, 142398, 142711, 142574, 142156, 143540, 144305,\n",
       "       145096, 145434, 145599, 146164, 146310, 146479, 146708, 147131,\n",
       "       146833, 145938, 146460, 146752, 147124, 147708, 148346, 148753],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4c7cc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "800ef67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout=0.3\n",
    "epoch=12\n",
    "batch_size=14\n",
    "verbose=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3f22686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [0] TEST: [1]\n",
      "X_pt_train:  [120088]\n",
      "y_pt_train:  [120295]\n",
      "TRAIN: [0 1] TEST: [2]\n",
      "X_pt_train:  [120088 120295]\n",
      "y_pt_train:  [119858]\n",
      "TRAIN: [0 1 2] TEST: [3]\n",
      "X_pt_train:  [120088 120295 119858]\n",
      "y_pt_train:  [120356]\n",
      "TRAIN: [0 1 2 3] TEST: [4]\n",
      "X_pt_train:  [120088 120295 119858 120356]\n",
      "y_pt_train:  [121078]\n",
      "TRAIN: [0 1 2 3 4] TEST: [5]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078]\n",
      "y_pt_train:  [121528]\n",
      "TRAIN: [0 1 2 3 4 5] TEST: [6]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528]\n",
      "y_pt_train:  [122206]\n",
      "TRAIN: [0 1 2 3 4 5 6] TEST: [7]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206]\n",
      "y_pt_train:  [122970]\n",
      "TRAIN: [0 1 2 3 4 5 6 7] TEST: [8]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970]\n",
      "y_pt_train:  [122796]\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8] TEST: [9]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796]\n",
      "y_pt_train:  [122465]\n",
      "TRAIN: [0 1 2 3 4 5 6 7 8 9] TEST: [10]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465]\n",
      "y_pt_train:  [123690]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10] TEST: [11]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690]\n",
      "y_pt_train:  [124336]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11] TEST: [12]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336]\n",
      "y_pt_train:  [124824]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12] TEST: [13]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824]\n",
      "y_pt_train:  [124628]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13] TEST: [14]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628]\n",
      "y_pt_train:  [124588]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] TEST: [15]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588]\n",
      "y_pt_train:  [125185]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15] TEST: [16]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185]\n",
      "y_pt_train:  [125747]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16] TEST: [17]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747]\n",
      "y_pt_train:  [126607]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17] TEST: [18]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607]\n",
      "y_pt_train:  [126203]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18] TEST: [19]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203]\n",
      "y_pt_train:  [127166]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19] TEST: [20]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166]\n",
      "y_pt_train:  [127229]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] TEST: [21]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229]\n",
      "y_pt_train:  [127447]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21] TEST: [22]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447]\n",
      "y_pt_train:  [128266]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22] TEST: [23]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266]\n",
      "y_pt_train:  [128593]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23] TEST: [24]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593]\n",
      "y_pt_train:  [129329]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24] TEST: [25]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329]\n",
      "y_pt_train:  [130058]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25] TEST: [26]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058]\n",
      "y_pt_train:  [129969]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26] TEST: [27]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969]\n",
      "y_pt_train:  [128325]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27] TEST: [28]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325]\n",
      "y_pt_train:  [129041]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28] TEST: [29]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041]\n",
      "y_pt_train:  [129476]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29] TEST: [30]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476]\n",
      "y_pt_train:  [129897]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30] TEST: [31]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897]\n",
      "y_pt_train:  [129605]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31] TEST: [32]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605]\n",
      "y_pt_train:  [129796]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32] TEST: [33]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796]\n",
      "y_pt_train:  [141249]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33] TEST: [34]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249]\n",
      "y_pt_train:  [141673]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34] TEST: [35]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673]\n",
      "y_pt_train:  [142398]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35] TEST: [36]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398]\n",
      "y_pt_train:  [142711]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36] TEST: [37]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711]\n",
      "y_pt_train:  [142574]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37] TEST: [38]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574]\n",
      "y_pt_train:  [142156]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38] TEST: [39]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156]\n",
      "y_pt_train:  [143540]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39] TEST: [40]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540]\n",
      "y_pt_train:  [144305]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40] TEST: [41]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305]\n",
      "y_pt_train:  [145096]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41] TEST: [42]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096]\n",
      "y_pt_train:  [145434]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42] TEST: [43]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434]\n",
      "y_pt_train:  [145599]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43] TEST: [44]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599]\n",
      "y_pt_train:  [146164]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44] TEST: [45]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164]\n",
      "y_pt_train:  [146310]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45] TEST: [46]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310]\n",
      "y_pt_train:  [146479]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46] TEST: [47]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479]\n",
      "y_pt_train:  [146708]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47] TEST: [48]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708]\n",
      "y_pt_train:  [147131]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48] TEST: [49]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131]\n",
      "y_pt_train:  [146833]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49] TEST: [50]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131 146833]\n",
      "y_pt_train:  [145938]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50] TEST: [51]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131 146833\n",
      " 145938]\n",
      "y_pt_train:  [146460]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51] TEST: [52]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131 146833\n",
      " 145938 146460]\n",
      "y_pt_train:  [146752]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52] TEST: [53]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131 146833\n",
      " 145938 146460 146752]\n",
      "y_pt_train:  [147124]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53] TEST: [54]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131 146833\n",
      " 145938 146460 146752 147124]\n",
      "y_pt_train:  [147708]\n",
      "TRAIN: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54] TEST: [55]\n",
      "X_pt_train:  [120088 120295 119858 120356 121078 121528 122206 122970 122796 122465\n",
      " 123690 124336 124824 124628 124588 125185 125747 126607 126203 127166\n",
      " 127229 127447 128266 128593 129329 130058 129969 128325 129041 129476\n",
      " 129897 129605 129796 141249 141673 142398 142711 142574 142156 143540\n",
      " 144305 145096 145434 145599 146164 146310 146479 146708 147131 146833\n",
      " 145938 146460 146752 147124 147708]\n",
      "y_pt_train:  [148346]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "i=1\n",
    "for train_index, test_index in tscv.split(X_pt):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_pt_train,  y_pt_train = X_pt[train_index], X_pt[test_index]\n",
    "    \n",
    "\n",
    "    print(\"X_pt_train: \",X_pt_train)\n",
    "    print(\"y_pt_train: \",y_pt_train)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07edd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    model.add(LSTM(55, activation='relu', input_shape=(1,1), return_sequences=True))\n",
    "    model.add(LSTM(20, activation=\"relu\", return_sequences=True))\n",
    "    model.add(Dropout(dropout)) \n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    \n",
    "    model_fit = model.fit(X_pt_train, y_pt_train,epochs=epoch,verbose=verbose)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2c4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #n+1일 값 찾기\n",
    "    last_X=np.concatenate((X_pt_train[-1][1:],np.array([y_pt_train[-1]])),axis=0)\n",
    "    last_Y = model.predict(np.array([last_X]))\n",
    "    \n",
    "    #n+1일 값 추가\n",
    "    new_X_train=np.concatenate((X_pt_train[:],np.array([last_X])),axis=0)\n",
    "    new_y_train=np.concatenate((y_pt_train[:],last_Y),axis=0)\n",
    "    \n",
    "    X_pt_train=np.array(new_X_pt_train)\n",
    "    y_pt_train=np.array(new_y_pt_train)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ab646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터 설정\n",
    "size=len(X_pt)\n",
    "dropout=0.3\n",
    "epoch=100\n",
    "batch_size=14\n",
    "verbose=1\n",
    "\n",
    "\n",
    "myIter=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2cbc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt=X_pt.reshape(1,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f031c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pt_train=X_pt_train.reshape(1,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d7f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt=y_pt.reshape(1,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pt_train=y_pt_train.reshape(1,1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9303b112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ts_train_test_normalize(all_data, time_steps, for_periods):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        data: dataframe with dates and price data\n",
    "    output: \n",
    "        X_train, y_train: data from 2013/1/1-2018/12/31 \n",
    "        X_test : data from 2019- \n",
    "        sc :     insantiated MinMaxScaler object fit to the training data \n",
    "    \"\"\"\n",
    "    # create training and test set \n",
    "    ts_train = all_data[:'2018'].iloc[:,0:1].values\n",
    "    ts_test = all_data['2019':].iloc[:,0:1].values \n",
    "    ts_train_len = len(ts_train)\n",
    "    ts_test_len = len(ts_test)\n",
    "    \n",
    "    # scale the data \n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    sc = MinMaxScaler(feature_range=(0,1))\n",
    "    ts_train_scaled = sc.fit_transform(ts_train)\n",
    "    \n",
    "    # create training data of s samples and t time steps \n",
    "    X_train = [] \n",
    "    y_train = [] \n",
    "    for i in range(time_steps, ts_train_len-1):\n",
    "        X_train.append(ts_train_scaled[i-time_steps:i, 0])\n",
    "        y_train.append(ts_train_scaled[i:i+for_periods, 0])\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "    # Reshaping X_train for efficient modelling \n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1 ))\n",
    "    \n",
    "    inputs = pd.concat((all_data[\"Adj Close\"][:'2018'], all_data[\"Adj Close\"]['2019':]), axis=0).values\n",
    "    inputs = inputs[len(inputs)-len(ts_test)-time_steps:]\n",
    "    inputs = inputs.reshape(-1,1)\n",
    "    inputs = sc.transform(inputs)\n",
    "    \n",
    "    # Preparing X_test \n",
    "    X_test = [] \n",
    "    for i in range(time_steps, ts_test_len + time_steps - for_periods):\n",
    "        X_test.append(inputs[i-time_steps:i,0])\n",
    "    \n",
    "    X_test = np.array(X_test)\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    return X_train, y_train , X_test, sc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7d6aaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
